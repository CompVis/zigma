defaults:
  - data: in256
  - train: default
  - dist: default
  - model: default
  - wandb: default
  - optim: default
  - ode: ode
  - sde: sde
  - _self_



mixed_precision: null #fp8, fp16, fp32, no

ckpt: null  

global_seed: 0
log_every: 100
ckpt_every: 50_000
sample_every: 10_000
max_grad_norm: 2.0



use_wandb: true
note: note   
timestamp: 

ema_rate: 0.9999
is_latent: true 
use_latent: false


sample_mode: null #ODE or SDE


# sampling
likelihood: false 
allow_tf32: false  # True: fast but may lead to some small numerical differences
sample_dir: samples
offline_sample_local_bs: 4
num_fid_samples: 4000 #50_000
sample_debug: false 


vae: ema #", type=str, choices=["ema", "mse"], default="ema")  # Choice doesn't affect training
cfg_scale: 1.0 #4.0


hydra:
  run:
    dir: outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}_${timestamp} #_${hydra.job.id}
    #dir: outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}_${now}
  job:
    name: ${model.name}_${data.name}_bs${data.batch_size}







